{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interpretability\n",
    "\n",
    "> Recipes for Model Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM, GAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "#### Assumptions\n",
    "1. Target $y$ and features $X$ have a linear relationship (else not modelable)\n",
    "2. No correlation between features in $X$ (else coefficients have numerical instability)\n",
    "3. Residuals are normally distributed\n",
    "4. Trendline is homoscedastic -- variance is the same everywhere on the line\n",
    "\n",
    "#### Interpretation\n",
    "- Product of coefficient and feature value $c_1x_1$ is the marginal contribution of $x_1$ in terms of $y$\n",
    "- Sum up products to get the estimated value for target $y = \\sum{c_ix_i}_{i=0}^n$\n",
    "- Interaction terms $x_ix_j$ allow for two variables to have a non-additive effect on the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "#### Assumptions\n",
    "1. Target $y$ and log-odds of features $X$ have a linear relationship\n",
    "2. No correlation between features in $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM-like models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "- Read the tree\n",
    "- Single tree prone to overfitting; do not use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP\n",
    "- Maybe better to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "# train an XGBoost model\n",
    "X, y = shap.datasets.boston()\n",
    "model = xgboost.XGBRegressor().fit(X, y)\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# visualize the first prediction's explanation\n",
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Agnostic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP\n",
    "- Maybe better to use [DALEX](https://github.com/ModelOriented/DALEX), which includes a SHAP implementation.  It is better maintained at this point than the [original SHAP from slundberg](https://github.com/slundberg/shap) (over 1k issues open)\n",
    "- Uses a game theory to attribute predictions to features in a model\n",
    "- Seems to be used extensively now, but some debate on soundness:\n",
    "  - https://arxiv.org/pdf/2002.11097.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Models (TensorFlow/PyTorch)\n",
    "\n",
    "#### DeepExplainer\n",
    "\n",
    "#### GradientExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Inference Models"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
