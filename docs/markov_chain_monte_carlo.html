---

title: Markov Chain Monte Carlo (MCMC)


keywords: fastai
sidebar: home_sidebar

summary: "Recipes for MCMC"
description: "Recipes for MCMC"
nb_path: "nbs/07_concepts/markov_chain_monte_carlo.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/07_concepts/markov_chain_monte_carlo.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Overview">Overview<a class="anchor-link" href="#Overview"> </a></h2><h3 id="Goal">Goal<a class="anchor-link" href="#Goal"> </a></h3><ul>
<li>Sample from some distribution $p(x) = \frac{f(x)}{NC}$, but we don't know the equation for $p(x)$, only $f(x)$.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Why-not-use-Accept-Reject?">Why not use Accept-Reject?<a class="anchor-link" href="#Why-not-use-Accept-Reject?"> </a></h3><ul>
<li>In Accept-Reject, we:<ul>
<li>Choose some function $g(x)$ that is close to the distribution we want to sample $p(x)$</li>
<li>Sample from $g(x)$ and accept with probability $\frac{f(x)}{Mg(x)}$</li>
<li>Intuitively, the ratio $\frac{f(x)}{g(x)}$ tells us that we should prefer samples with high densities (big $f(x)$, likely to come from $p(x)$) or are rare candidates (low $g(x)$)</li>
</ul>
</li>
<li>For high dimensional data, it is hard to efficiently sample from $Mg(x) &gt; f(x)$ because $M$ tends to be very large.<ul>
<li>Large M results in low acceptance probabilties (because $\frac{1}{M} \propto \text{acceptance}$)</li>
<li>This means we need to sample a lot to get accepted samples.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Aside-(math)">Aside (math)<a class="anchor-link" href="#Aside-(math)"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary data-open="Hide Code" data-close="Show Code"></summary>
        <summary></summary>
        <div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Math</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">\begin</span><span class="si">{align}</span><span class="s1"></span>
<span class="s1">\\NC = \int_{-\infty}^{\infty}f(x)dx</span>
<span class="s1">\\</span>
<span class="s1">\\\text{Bayes Theorem: } P(S|A) &amp;= \frac{P(A|S)D(S)}{P(A)}</span>
<span class="s1">\\&amp;=\frac{\frac{f(x)}{Mg(x)} \times g(x)}{P(A)}</span>
<span class="s1">\\</span>
<span class="s1">\\\text</span><span class="si">{rearrange: }</span><span class="s1"> P(A) &amp;= \int_{-\infty}^{\infty}g(x)\frac{f(x)}{Mg(x)}dx</span>
<span class="s1">\\&amp;=\frac</span><span class="si">{1}{M}</span><span class="s1">\int_{-\infty}^{\infty}f(x)dx</span>
<span class="s1">\\&amp;=\frac</span><span class="si">{NC}{M}</span><span class="s1"></span>
<span class="s1">\\\text</span><span class="si">{rearrange: }</span><span class="s1"> P(S|A) &amp;= \frac{f(x)/M}{NC/M}</span>
<span class="s1">\\&amp;=\frac{f(x)}</span><span class="si">{NC}</span><span class="s1"></span>
<span class="s1">\\&amp;=p(x)</span>
<span class="s1">\end</span><span class="si">{align}</span><span class="s1"></span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_latex output_subarea ">
$\displaystyle 
\begin{align}
\\NC = \int_{-\infty}^{\infty}f(x)dx
\\
\\\text{Bayes Theorem: } P(S|A) &= \frac{P(A|S)D(S)}{P(A)}
\\&=\frac{\frac{f(x)}{Mg(x)} \times g(x)}{P(A)}
\\
\\\text{rearrange: } P(A) &= \int_{-\infty}^{\infty}g(x)\frac{f(x)}{Mg(x)}dx
\\&=\frac{1}{M}\int_{-\infty}^{\infty}f(x)dx
\\&=\frac{NC}{M}
\\\text{rearrange: } P(S|A) &= \frac{f(x)/M}{NC/M}
\\&=\frac{f(x)}{NC}
\\&=p(x)
\end{align}
$
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Solution">Solution<a class="anchor-link" href="#Solution"> </a></h3><ul>
<li>Build a markov chain that has a stationary distribution that represents $p(x)$, then sample from it.<ul>
<li>Each draw from the distribution is dependent on the previous draw (Markov Chain)</li>
<li>Simulate draws from the markov chain (Monte Carlo)</li>
<li>Requires drawing from the markov chain for some "burn-in" period before converging to $p(x)$.</li>
</ul>
</li>
</ul>
<p><a href="https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggTFJcblxuc3ViZ3JhcGggYnVybi1pblxuICAgIFhfMCAtLT4gWF8xIC0tPiBYXy4uLlxuZW5kXG5cbnN1YmdyYXBoIFwicCh4KVwiXG4gICAgWF8uLi4gLS0-IFhfQiAtLT4gWF9CKzEgLS0-IFhfQisuLi5cbmVuZCIsIm1lcm1haWQiOnt9LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ"><img src="https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggTFJcblxuc3ViZ3JhcGggYnVybi1pblxuICAgIFhfMCAtLT4gWF8xIC0tPiBYXy4uLlxuZW5kXG5cbnN1YmdyYXBoIFwicCh4KVwiXG4gICAgWF8uLi4gLS0-IFhfQiAtLT4gWF9CKzEgLS0-IFhfQisuLi5cbmVuZCIsIm1lcm1haWQiOnt9LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ" alt=""></a></p>
<ul>
<li>To guarantee that the markov chain converges to $p(x)$, you need to satisfy the detailed balance condition:</li>
</ul>
<ol>
<li><p>$y \rarr x$.  $\forall{x,y} p(x)T(y|x) = p(y)T(x|y)$  --  The probability of going $x \rarr y$ is the same as the probability of going</p>
</li>
<li><p>$\sum{p(x)T(y|x)} = \sum{p(y)T(x|y)}$  --  Therefore the sums of those probabilities is the same</p>
</li>
<li><p>$pT = p$  --  Therefore we reach steady state</p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Metropolis-Hastings">Metropolis-Hastings<a class="anchor-link" href="#Metropolis-Hastings"> </a></h2><p>The transition probability is equal to your likelihood of going from A to B times your likelihood of going from B to A.
$$
\\ \forall{a,b}
\\ p(a)T(a\rarr b) = p(b)T(b\rarr a)
\\ \frac{f(a)}{NC}g(b|a)A(a\rarr b) = \frac{f(b)}{NC}g(a|b)A(b\rarr a)
\\ \frac{A(a\rarr b)}{A(b\rarr a)} = \frac{f(b)}{f(a)} \times \frac{g(a|b)}{g(b|a)}
$$</p>
<p>Here, we note that $r_f = \frac{f(b)}{f(a)}$ and $r_g = \frac{g(a|b)}{g(b|a)}$</p>
<p>For $r_fr_g &lt;1$, $A(a\rarr b) = r_fr_g$, and $A(b\rarr a) = 1$
For $r_fr_g \ge1$, $A(a\rarr b) = 1$, and $A(b\rarr a) = \frac{1}{r_fr_g}$</p>
<p>Therefore, $A(a\rarr b) = \min{(1, r_fr_g)}$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>At each step, we will sample some function $g(x_{t+1}|x_t)$.  This function can be any distribution, e.g. $\mathcal{N}(x_t, \sigma^2)$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Gibbs-Sampling">Gibbs Sampling<a class="anchor-link" href="#Gibbs-Sampling"> </a></h2><ul>
<li>Useful for multidimensional sampling</li>
</ul>

</div>
</div>
</div>
</div>
 

